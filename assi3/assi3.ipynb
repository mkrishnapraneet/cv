{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use ffmpeg to convert the video to a series of images\n",
    "# def extract_frames(video_path, output_dir, duration=30):\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Run ffmpeg command to extract frames\n",
    "#     command = [\n",
    "#         'ffmpeg',\n",
    "#         '-i', video_path,\n",
    "#         # '-vf', 'fps=1/1',\n",
    "#         '-t', str(duration),\n",
    "#         os.path.join(output_dir, 'frame_%04d.png')\n",
    "#     ]\n",
    "#     subprocess.run(command)\n",
    "\n",
    "# video_path = './ForrestGump.mp4'\n",
    "# output_dir = 'frames'\n",
    "# extract_frames(video_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use viola jones to detect faces in the images\n",
    "face_rectangles = []\n",
    "def detect_faces(image_path, face_cascade, faces_dir='faces'):\n",
    "    # Load the cascade\n",
    "\n",
    "    # Read the input image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Convert into grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "    # Draw rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    # save the face rectangles\n",
    "    face_rectangles.append(faces)\n",
    "\n",
    "    # Save the output\n",
    "    faces_path = os.path.join(faces_dir, os.path.basename(image_path))\n",
    "    cv2.imwrite(faces_path, img)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# detect faces in all the images and save the results in another directory\n",
    "frames_dir = 'frames'\n",
    "faces_dir = 'faces'\n",
    "os.makedirs(faces_dir, exist_ok=True)\n",
    "frame_files = os.listdir(frames_dir)\n",
    "frame_files.sort()\n",
    "\n",
    "for frame in frame_files:\n",
    "    frame_path = os.path.join(frames_dir, frame)\n",
    "    detect_faces(frame_path, face_cascade, faces_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "The code takes a total of 36 seconds to detect faces in all 720 images. This means that it takes 0.05 seconds on average to detect faces in one image.\n",
    "The factors that affect the speed of the face detection algorithm are:\n",
    "1. Scale Factor: The scaleFactor parameter used in the detectMultiScale function determines how much the image size is reduced at each image scale. Smaller values lead to slower but more accurate detection, while larger values speed up detection but may miss smaller faces.\n",
    "\n",
    "2. Minimum Neighbors: The minNeighbors parameter specifies how many neighbors each candidate rectangle should have to retain it. Higher values increase accuracy but also increase processing time.\n",
    "The number of weak classifiers being used\n",
    "3. The number of features being used and the number of weak classifiers being used.\n",
    "4. The size of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ffmpeg to convert the images back to a video\n",
    "def create_video(frames_dir, output_path, fps=24):\n",
    "    # Run ffmpeg command to create video\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-framerate', str(fps),\n",
    "        '-i', os.path.join(frames_dir, 'frame_%04d.png'),\n",
    "        '-c:v', 'libx264',\n",
    "        '-pix_fmt', 'yuv420p',\n",
    "        output_path\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "\n",
    "output_path = 'output.mp4'\n",
    "create_video(faces_dir, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "Here is the link to the video with detected faces:\n",
    "[Link to the video](https://iiitaphyd-my.sharepoint.com/:v:/g/personal/mulukutla_p_research_iiit_ac_in/EbxlMeSlbIVMm458a1FtfnEB_AyQYyxWw5OPynz6ggAYkg?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=8b52RS)\n",
    "\n",
    "Observations:\n",
    "1. The face detection algorithm is able to detect faces very accurately if they are facing the camera directly. However, it struggles to detect faces that are not facing the camera directly or are partially occluded. Though side faces are detected sometimes, it is not reliable.\n",
    "2. The algorithm detects a lot of non-faces as faces in the video. This is because the algorithm is not able to differentiate between faces and objects that look similar to faces. This mostly happens when the object is slightly textured similar to a face. That is, the Haar cascades only look at the sum of pixel intensities in the region and not the texture of the region, which leads to false positives. Some examples are some leaf patterns in the background or Forrest's shirt patterns being detected as faces.\n",
    "3. The algorithm is not able to detect faces that are a little far away from the camera, because the eyes and other features that are used by the Haaar cascades to detect faces are not clearly visible in the image.\n",
    "4. It is also not able to detect faces when the face is visible, but is upside down. This also happens with faces that are tilted. This is because the Haar cascades are created to identify faces in rectangles, so tilted faces are not detected.\n",
    "5. A good feature is that it is able to detect faces even when they are slightly blurred, but the features are still somewhat visible. This is again because the Haar cascades only look at the sum of pixel intensities in the region and not the texture of the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of face tracks :  273\n"
     ]
    }
   ],
   "source": [
    "class FaceTracker:\n",
    "    def __init__(self, start_frame, face_rect):\n",
    "        self.start_frame = start_frame\n",
    "        self.end_frame = start_frame\n",
    "        self.face_rect = face_rect\n",
    "        self.track = [face_rect]\n",
    "\n",
    "    def update(self, frame_number, new_face_rect):\n",
    "        iou = calculate_iou(self.face_rect, new_face_rect)\n",
    "        if iou > 0.5:\n",
    "            self.end_frame = frame_number\n",
    "            self.face_rect = new_face_rect\n",
    "            self.track.append(new_face_rect)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def calculate_iou(rect1, rect2):\n",
    "    # Calculate the intersection rectangle\n",
    "    xA = max(rect1[0], rect2[0])\n",
    "    yA = max(rect1[1], rect2[1])\n",
    "    xB = min(rect1[0] + rect1[2], rect2[0] + rect2[2])\n",
    "    yB = min(rect1[1] + rect1[3], rect2[1] + rect2[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # Compute the area of both rectangles\n",
    "    boxAArea = rect1[2] * rect1[3]\n",
    "    boxBArea = rect2[2] * rect2[3]\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "# Create face trackers based on face detections in consecutive frames\n",
    "def create_face_tracks(face_rectangles):\n",
    "    face_tracks = []\n",
    "    active_tracks = []\n",
    "\n",
    "    for i, faces in enumerate(face_rectangles):\n",
    "        for face in faces:\n",
    "            face_found = False\n",
    "            for track in active_tracks:\n",
    "                if track.update(i, face):\n",
    "                    face_found = True\n",
    "                    break\n",
    "            if not face_found:\n",
    "                new_track = FaceTracker(i, face)\n",
    "                active_tracks.append(new_track)\n",
    "\n",
    "        # End existing tracks if faces are not visible in the next frame\n",
    "        for track in active_tracks:\n",
    "            if track.end_frame == i - 1:\n",
    "                face_tracks.append(track)\n",
    "                active_tracks.remove(track)\n",
    "\n",
    "    # Add remaining active tracks\n",
    "    face_tracks.extend(active_tracks)\n",
    "\n",
    "    return face_tracks\n",
    "\n",
    "# Assuming face_rectangles is a list of lists containing face detections in consecutive frames\n",
    "face_tracks = create_face_tracks(face_rectangles)\n",
    "\n",
    "# Print face tracks\n",
    "# for track in face_tracks:\n",
    "#     print(\"Start frame:\", track.start_frame)\n",
    "#     print(\"End frame:\", track.end_frame)\n",
    "#     print(\"Face rectangle:\", track.face_rect)\n",
    "#     print()\n",
    "\n",
    "print(\"Number of face tracks : \", len(face_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate the face tracks in the video using integer labels and save the video\n",
    "def indicate_face_tracks(image_path, face_tracks, frame_num, tracks_dir):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Draw rectangle around the faces if they are part of a track\n",
    "    for i, track in enumerate(face_tracks):\n",
    "        if frame_num >= track.start_frame and frame_num <= track.end_frame:\n",
    "            # print(frame_num - track.start_frame)\n",
    "            if (frame_num - track.start_frame) >= len(track.track):\n",
    "                continue\n",
    "            (x, y, w, h) = track.track[frame_num - track.start_frame]\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(img, str(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Save the output\n",
    "    tracks_path = os.path.join(tracks_dir, os.path.basename(image_path))\n",
    "    cv2.imwrite(tracks_path, img)\n",
    "\n",
    "tracks_dir = 'tracks'\n",
    "os.makedirs(tracks_dir, exist_ok=True)\n",
    "\n",
    "for i, frame in enumerate(frame_files):\n",
    "    frame_path = os.path.join(frames_dir, frame)\n",
    "    indicate_face_tracks(frame_path, face_tracks, i, tracks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from 'tracks/frame_%04d.png':\n",
      "  Duration: 00:00:30.00, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc), 854x480, 24 fps, 24 tbr, 24 tbn, 24 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x64e9eb6745c0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x64e9eb6745c0] profile High, level 3.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x64e9eb6745c0] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'tracks.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 854x480, q=2-31, 24 fps, 12288 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=  720 fps=302 q=-1.0 Lsize=    4388kB time=00:00:29.87 bitrate=1203.3kbits/s speed=12.5x    \n",
      "video:4379kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.208019%\n",
      "[libx264 @ 0x64e9eb6745c0] frame I:15    Avg QP:18.20  size: 45110\n",
      "[libx264 @ 0x64e9eb6745c0] frame P:211   Avg QP:23.60  size: 10247\n",
      "[libx264 @ 0x64e9eb6745c0] frame B:494   Avg QP:26.91  size:  3330\n",
      "[libx264 @ 0x64e9eb6745c0] consecutive B-frames:  6.0%  5.6%  6.2% 82.2%\n",
      "[libx264 @ 0x64e9eb6745c0] mb I  I16..4: 19.0% 25.3% 55.7%\n",
      "[libx264 @ 0x64e9eb6745c0] mb P  I16..4:  1.8%  8.8%  5.3%  P16..4: 30.7% 11.2%  6.6%  0.0%  0.0%    skip:35.6%\n",
      "[libx264 @ 0x64e9eb6745c0] mb B  I16..4:  0.4%  1.4%  1.1%  B16..8: 36.7%  5.2%  1.8%  direct: 1.5%  skip:52.0%  L0:48.3% L1:45.8% BI: 5.9%\n",
      "[libx264 @ 0x64e9eb6745c0] 8x8 transform intra:46.4% inter:58.9%\n",
      "[libx264 @ 0x64e9eb6745c0] coded y,uvDC,uvAC intra: 69.7% 69.4% 25.5% inter: 11.4% 8.6% 1.3%\n",
      "[libx264 @ 0x64e9eb6745c0] i16 v,h,dc,p: 34% 37% 11% 18%\n",
      "[libx264 @ 0x64e9eb6745c0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 18% 23% 13%  6%  7%  8%  8%  8%  9%\n",
      "[libx264 @ 0x64e9eb6745c0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 18% 26% 13%  6%  7%  7%  7%  7%  7%\n",
      "[libx264 @ 0x64e9eb6745c0] i8c dc,h,v,p: 42% 30% 19%  9%\n",
      "[libx264 @ 0x64e9eb6745c0] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x64e9eb6745c0] ref P L0: 68.7% 13.3% 12.8%  5.1%\n",
      "[libx264 @ 0x64e9eb6745c0] ref B L0: 90.0%  7.8%  2.2%\n",
      "[libx264 @ 0x64e9eb6745c0] ref B L1: 96.0%  4.0%\n",
      "[libx264 @ 0x64e9eb6745c0] kb/s:1195.61\n"
     ]
    }
   ],
   "source": [
    "# convert the images in the tracks directory to a video\n",
    "output_path = 'tracks.mp4'\n",
    "create_video(tracks_dir, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4\n",
    "1. As seen above, 273 unique face tracks were obtained using the specified criterion. \n",
    "2. [Link to face tracks video](https://iiitaphyd-my.sharepoint.com/:v:/g/personal/mulukutla_p_research_iiit_ac_in/EcZV__mKAQJGmN92GFIe1hUBWzxrBd9iiEilNnCZ3an_wA?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=zODYfe)\n",
    "3. Different people do not get associated in one track, but the same person is present in different tracks in many places. This can be due to occlusion, or the face moving too fast for the IOU to associate it with the same track. Some example timestamps are : from 0:15 - 0:17, when the kid is about to throw the rock, his face moves across the screen, thus creating a new track at almost every frame. Another example is around 0:22, where a kid on the cycle moves across the screen, occluding the girl's face for a few frames. This results in a new track for the girl's face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
