{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a subset of the dataset with 100 images and their labels\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def create_subset(input_images_dir, input_labels_dir, output_images_dir, output_labels_dir, subset_size):\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(output_images_dir, exist_ok=True)\n",
    "    os.makedirs(output_labels_dir, exist_ok=True)\n",
    "\n",
    "    # List all images in the input directory\n",
    "    images = os.listdir(input_images_dir)\n",
    "\n",
    "    # Choose a random subset of images\n",
    "    subset_images = random.sample(images, subset_size)\n",
    "\n",
    "    # Copy selected images and their corresponding labels to the output directory\n",
    "    for image in subset_images:\n",
    "        # Copy image\n",
    "        shutil.copy(os.path.join(input_images_dir, image), output_images_dir)\n",
    "        \n",
    "        # Corresponding label file (assuming label filename is same as image filename with a different extension)\n",
    "        label_file = os.path.splitext(image)[0] + '.txt'\n",
    "        shutil.copy(os.path.join(input_labels_dir, label_file), output_labels_dir)\n",
    "\n",
    "# Example usage\n",
    "input_images_dir = 'datasets/images/train'\n",
    "input_labels_dir = 'datasets/labels/train'\n",
    "output_images_dir = 'datasets/images/train_subset'\n",
    "output_labels_dir = 'datasets/labels/train_subset'\n",
    "subset_size = 100\n",
    "\n",
    "create_subset(input_images_dir, input_labels_dir, output_images_dir, output_labels_dir, subset_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = YOLO('yolov8n.yaml')       # Initialize model from scratch\n",
    "model2 = YOLO('yolov8n.pt')         # Initialize model from existing weights\n",
    "model3 = YOLO('yolov8m.pt')         # Initialize model from existing weights but medium size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = model1.train(data='./datasets/config2.yaml', epochs=20, imgsz=640)\n",
    "# results2 = model1.train(data='./datasets/config.yaml', epochs=20, imgsz=640)\n",
    "# results3 = model2.train(data='./datasets/config2.yaml', epochs=20, imgsz=640)\n",
    "# results4 = model2.train(data='./datasets/config.yaml', epochs=20, imgsz=640)\n",
    "# results5 = model3.train(data='./datasets/config2.yaml', epochs=20, imgsz=640)\n",
    "# results6 = model3.train(data='./datasets/config.yaml', epochs=20, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in YOLOv8M: 25902640\n",
      "Number of convolutional layers in YOLOv8M: 84\n"
     ]
    }
   ],
   "source": [
    "# Function to count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Function to count convolutional layers\n",
    "def count_conv_layers(model):\n",
    "    return sum(1 for m in model.modules() if isinstance(m, torch.nn.Conv2d))\n",
    "\n",
    "# Calculate parameters and convolutional layers\n",
    "num_parameters = count_parameters(model3)\n",
    "num_conv_layers = count_conv_layers(model3)\n",
    "\n",
    "print(f\"Number of parameters in YOLOv8M: {num_parameters}\")\n",
    "print(f\"Number of convolutional layers in YOLOv8M: {num_conv_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.46 🚀 Python-3.10.12 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "WARNING ⚠️ Upgrade to torch>=2.0.0 for deterministic training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=./datasets/config2.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train28, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=20, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=0.0, translate=0.0, scale=0.0, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.0, bgr=0.0, mosaic=0.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.0, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train28\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train28', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulukutla-p\u001b[0m (\u001b[33mmkp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/prani/academics/fourth_year_sem2/cv/assi3/wandb/run-20240411_225532-sz4vs5im</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkp/YOLOv8/runs/sz4vs5im' target=\"_blank\">train28</a></strong> to <a href='https://wandb.ai/mkp/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkp/YOLOv8' target=\"_blank\">https://wandb.ai/mkp/YOLOv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkp/YOLOv8/runs/sz4vs5im' target=\"_blank\">https://wandb.ai/mkp/YOLOv8/runs/sz4vs5im</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/prani/academics/fourth_year_sem2/cv/assi3/datasets/labels/train_subset.cache... 381 images, 0 backgrounds, 0 corrupt: 100%|██████████| 381/381 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/prani/academics/fourth_year_sem2/cv/assi3/datasets/labels/val.cache... 50 images, 0 backgrounds, 0 corrupt: 100%|██████████| 50/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train28/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train28\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      2.13G      1.139      3.361      1.337         13        640: 100%|██████████| 24/24 [00:03<00:00,  6.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52     0.0034      0.981      0.577      0.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      2.11G      1.056      2.858      1.288         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.727     0.0524      0.416      0.188\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      2.09G      1.059      2.678      1.289         14        640: 100%|██████████| 24/24 [00:02<00:00,  9.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52       0.54      0.271      0.377      0.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      2.09G      1.059      2.392      1.292         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.292      0.327      0.224       0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      2.09G     0.9623      2.136      1.202         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.527      0.519      0.444      0.222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      2.09G     0.8659       1.78      1.118         14        640: 100%|██████████| 24/24 [00:02<00:00,  9.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.654      0.481       0.52      0.306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      2.09G     0.7692      1.451      1.058         14        640: 100%|██████████| 24/24 [00:02<00:00,  9.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52       0.53        0.5       0.49      0.313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      2.09G     0.6645      1.212     0.9906         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52       0.63      0.558      0.561      0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      2.09G     0.6054      1.031     0.9584         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 12.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.505      0.327      0.412      0.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      2.09G     0.5569     0.9554     0.9268         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52       0.76      0.481      0.594      0.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      2.09G     0.4766     0.7744     0.8897         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.696      0.538      0.609      0.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      2.09G     0.4405     0.6951     0.8741         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.664      0.596      0.568      0.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      2.09G     0.4113     0.6481     0.8599         15        640: 100%|██████████| 24/24 [00:02<00:00,  9.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 12.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.749      0.596      0.624      0.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      2.09G     0.3728     0.5703     0.8458         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.667      0.654      0.668      0.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      2.09G     0.3387     0.5048     0.8356         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.776      0.596      0.636      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      2.09G     0.3104      0.465     0.8253         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.733      0.577      0.662      0.476\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      2.09G     0.2752     0.4225     0.8154         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 12.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.826      0.638       0.69      0.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      2.09G     0.2361     0.3729     0.8065         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 12.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.808      0.647      0.693      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      2.09G     0.2025     0.3417     0.7993         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52       0.79       0.65      0.693      0.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      2.09G     0.1727     0.3167      0.795         13        640: 100%|██████████| 24/24 [00:02<00:00,  9.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52       0.79      0.654      0.685      0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.018 hours.\n",
      "Optimizer stripped from runs/detect/train28/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train28/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train28/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.46 🚀 Python-3.10.12 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50         52      0.822      0.635       0.69      0.512\n",
      "Speed: 1.1ms preprocess, 1.4ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train28\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▃▅▆██▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>lr/pg1</td><td>▃▅▆██▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>lr/pg2</td><td>▃▅▆██▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▆▄▃▁▄▅▅▆▄▇▇▆▇█▇█████</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▆▂▂▁▃▄▄▅▃▆▆▆▇▇▇▇████</td></tr><tr><td>metrics/precision(B)</td><td>▁▇▆▃▅▇▅▆▅▇▇▇▇▇█▇████</td></tr><tr><td>metrics/recall(B)</td><td>█▁▃▃▅▄▄▅▃▄▅▅▅▆▅▅▅▅▆▅</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>█▇▇▇▇▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/cls_loss</td><td>█▇▆▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/dfl_loss</td><td>█▇▇▇▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val/box_loss</td><td>▁▅█▇█▇▆▅█▄▅▄▃▄▃▃▃▃▂▂</td></tr><tr><td>val/cls_loss</td><td>▇█▆▆▆▅▃▂▃▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>val/dfl_loss</td><td>▁▄█▇█▇▄▄▆▄▄▃▃▃▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00012</td></tr><tr><td>lr/pg1</td><td>0.00012</td></tr><tr><td>lr/pg2</td><td>0.00012</td></tr><tr><td>metrics/mAP50(B)</td><td>0.69004</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.51154</td></tr><tr><td>metrics/precision(B)</td><td>0.82155</td></tr><tr><td>metrics/recall(B)</td><td>0.63462</td></tr><tr><td>model/GFLOPs</td><td>8.194</td></tr><tr><td>model/parameters</td><td>3011043</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>2.6</td></tr><tr><td>train/box_loss</td><td>0.17266</td></tr><tr><td>train/cls_loss</td><td>0.3167</td></tr><tr><td>train/dfl_loss</td><td>0.79503</td></tr><tr><td>val/box_loss</td><td>0.99274</td></tr><tr><td>val/cls_loss</td><td>1.69058</td></tr><tr><td>val/dfl_loss</td><td>1.34661</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train28</strong> at: <a href='https://wandb.ai/mkp/YOLOv8/runs/sz4vs5im' target=\"_blank\">https://wandb.ai/mkp/YOLOv8/runs/sz4vs5im</a><br/>Synced 5 W&B file(s), 19 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240411_225532-sz4vs5im/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "augmentations = {\n",
    "    'hsv_h': 0.0,\n",
    "    'hsv_s': 0.0,\n",
    "    'hsv_v': 0.0,\n",
    "    'degrees': 0.0,\n",
    "    'translate': 0.0,\n",
    "    'scale': 0.0,\n",
    "    'shear': 0.0,\n",
    "    'perspective': 0.0,\n",
    "    'flipud': 0.0,\n",
    "    'fliplr': 0.0,\n",
    "    'mosaic': 0.0,\n",
    "    'mixup': 0.0,\n",
    "    'copy_paste': 0.0,\n",
    "    'erasing': 0.0,\n",
    "}\n",
    "no_aug_results = model2.train(data='./datasets/config2.yaml', epochs=20, imgsz=640, close_mosaic=20, **augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "The dataset is divided into train and val partitions in images and labels directories. The corresponding images and labels can be identified by the same name given to them. The images are in .jpg format and the labels are in .txt format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "YOLO (You Only Look Once) is a real-time object detection algorithm. It differs fundamentally from R-CNNs and similar algorithms as it requires only a single pass of the image to make the detection, unlike the other algorithms which require multiple passes. The other algorithms first consider proposal regions. But YOLO has convolutional layers followed by fully connected layers. \n",
    "\n",
    "In YOLO, a single CNN is used to directly predict the class labels and bounding boxes of objects within an image. These models are trained end-to-end using a large dataset of labeled images and their associated object-bounding boxes. They are therefore much faster than R-CNNs and its improvements. Also, YOLO are often pretrained on ImageNet or other datasets, so their performance is often better than expected.\n",
    "\n",
    "YOLO\n",
    "The image is divided into grids, and each grid cell predicts B bounding boxes and confidence scores for those boxes. YOLO models use non-maximum suppression (NMS). NMS is used to identify and remove redundant or incorrect bounding boxes and to output a single bounding box for each object in the image. This gets rid of overlapping bounding boxes for the same object.\n",
    "\n",
    "YOLOv4\n",
    "The main improvement in YOLO v4 over previous versions is the use of a new CNN architecture called CSPNet. It is a variant of the ResNet architecture for detection. YOLO v4 uses k-means clustering to generate anchor boxes. This groups the ground truth bounding boxes into clusters and then using the centroids of the clusters as the anchor boxes. This allows the anchor boxes to be more closely aligned with the detected objects' size and shape.\n",
    "\n",
    "YOLOv7\n",
    "A key improvement in YOLO v7 is the use of a new loss function called focal loss. Previous versions of YOLO used a standard cross-entropy loss function, which is known to be less effective at detecting small objects. Focal loss down-weighs the loss for well-classified examples and focuses on the hard examples. This helps the model to focus on the difficult examples and improve its performance on small objects. YOLO v7 also has a higher resolution than the previous versions. It processes images at a resolution of 608 by 608 pixels, which is higher than the 416 by 416 resolution used in previous versions. This allows the model to detect smaller objects more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "YOLOv8n : 3.2 million parameters and 64 conv layers\n",
    "\n",
    "YOLOv8m : 25.9 million parameters and 84 conv layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4\n",
    "[Link to the Report containing the results](https://api.wandb.ai/links/mkp/16f9hq8k)\n",
    "\n",
    "(iii) The increase in dataset size mattered a lot more for the untrained model, whereas there was not much difference in the pretrained models. So, increased training time without a significant increase in map. Also, the YOLOv8m did not perform better than the nano version. I believe this is because the dataset is not large enough to benefit from the increased complexity of the model. If we had a larger and more varied dataset, we definitely would have seen better performance from the larger model.\n",
    "\n",
    "(iv) The comparison is done in the report linked above. I observe that the larger model tends to have multiple boxes on the same duck sometimes, compared to the smaller one. This is because the larger model is more complex and can detect more features, but it is not always beneficial. The smaller model is more consistent in its predictions and is more likely to predict the same box for the same duck across different epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5\n",
    "(i) Yes, the default run uses augmentations. The augmentations are listed [here](https://docs.ultralytics.com/modes/train/#augmentation-settings-and-hyperparameters). The augmentations are used to increase the diversity of the dataset and help the model generalize better. For example, scale parameter scales the image by a gain factor, simulating objects at different distances from the camera.\n",
    "\n",
    "(ii) [Link to the Report containing the results](https://api.wandb.ai/links/mkp/16f9hq8k). This contains the comparison of the model I trained with minimal augmentations. I did this on the best performing model from the previous section. The mAP50 decreased from 0.78 to 0.67 due to the lack of augmentations. This shows that augmentations are important for the model to generalize better and perform well on unseen data. The change can be said to be fairly significant. Because without augmentations, the model is not able to learn the features of the objects well enough to generalize to unseen data.\n",
    "\n",
    "(iii) I think hues and translate are two important augmentations. I arrived at this conclusion by trial and error. When I set the hues and a few other parameters to 0, I saw a drop in mAP, which remained same even when I changed some other augmentations. This shows that hues are an important augmentation. Also, Translate aids in learning to detect partially visible objects by translating the image horizontally and vertically by a fraction of the image size. Mosaic is also important because it combines four images into one, which increases the diversity of the dataset and helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
