{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use ffmpeg to convert the video to a series of images\n",
    "# def extract_frames(video_path, output_dir, duration=30):\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Run ffmpeg command to extract frames\n",
    "#     command = [\n",
    "#         'ffmpeg',\n",
    "#         '-i', video_path,\n",
    "#         # '-vf', 'fps=1/1',\n",
    "#         '-t', str(duration),\n",
    "#         os.path.join(output_dir, 'frame_%04d.png')\n",
    "#     ]\n",
    "#     subprocess.run(command)\n",
    "\n",
    "# video_path = './ForrestGump.mp4'\n",
    "# output_dir = 'frames'\n",
    "# extract_frames(video_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use viola jones to detect faces in the images\n",
    "# def detect_faces(image_path, face_cascade, faces_dir='faces'):\n",
    "#     # Load the cascade\n",
    "\n",
    "#     # Read the input image\n",
    "#     img = cv2.imread(image_path)\n",
    "\n",
    "#     # Convert into grayscale\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Detect faces\n",
    "#     faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "#     # Draw rectangle around the faces\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "#     # Save the output\n",
    "#     faces_path = os.path.join(faces_dir, os.path.basename(image_path))\n",
    "#     cv2.imwrite(faces_path, img)\n",
    "\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# # detect faces in all the images and save the results in another directory\n",
    "# frames_dir = 'frames'\n",
    "# faces_dir = 'faces'\n",
    "# os.makedirs(faces_dir, exist_ok=True)\n",
    "\n",
    "# for frame in os.listdir(frames_dir):\n",
    "#     frame_path = os.path.join(frames_dir, frame)\n",
    "#     detect_faces(frame_path, face_cascade, faces_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "The code takes a total of 36 seconds to detect faces in all 720 images. This means that it takes 0.05 seconds on average to detect faces in one image.\n",
    "The factors that affect the speed of the face detection algorithm are:\n",
    "1. Scale Factor: The scaleFactor parameter used in the detectMultiScale function determines how much the image size is reduced at each image scale. Smaller values lead to slower but more accurate detection, while larger values speed up detection but may miss smaller faces.\n",
    "\n",
    "2. Minimum Neighbors: The minNeighbors parameter specifies how many neighbors each candidate rectangle should have to retain it. Higher values increase accuracy but also increase processing time.\n",
    "The number of weak classifiers being used\n",
    "3. The number of features being used and the number of weak classifiers being used.\n",
    "4. The size of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use ffmpeg to convert the images back to a video\n",
    "# def create_video(frames_dir, output_path, fps=24):\n",
    "#     # Run ffmpeg command to create video\n",
    "#     command = [\n",
    "#         'ffmpeg',\n",
    "#         '-framerate', str(fps),\n",
    "#         '-i', os.path.join(frames_dir, 'frame_%04d.png'),\n",
    "#         '-c:v', 'libx264',\n",
    "#         '-pix_fmt', 'yuv420p',\n",
    "#         output_path\n",
    "#     ]\n",
    "#     subprocess.run(command)\n",
    "\n",
    "# output_path = 'output.mp4'\n",
    "# create_video(faces_dir, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "Here is the link to the video with detected faces:\n",
    "[Link to the video](https://iiitaphyd-my.sharepoint.com/:v:/g/personal/mulukutla_p_research_iiit_ac_in/EbxlMeSlbIVMm458a1FtfnEB_AyQYyxWw5OPynz6ggAYkg?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=8b52RS)\n",
    "\n",
    "Observations:\n",
    "1. The face detection algorithm is able to detect faces very accurately if they are facing the camera directly. However, it struggles to detect faces that are not facing the camera directly or are partially occluded. Though side faces are detected sometimes, it is not reliable.\n",
    "2. The algorithm detects a lot of non-faces as faces in the video. This is because the algorithm is not able to differentiate between faces and objects that look similar to faces. This mostly happens when the object is slightly textured similar to a face. That is, the Haar cascades only look at the sum of pixel intensities in the region and not the texture of the region, which leads to false positives. Some examples are some leaf patterns in the background or Forrest's shirt patterns being detected as faces.\n",
    "3. The algorithm is not able to detect faces that are a little far away from the camera, because the eyes and other features that are used by the Haaar cascades to detect faces are not clearly visible in the image.\n",
    "4. It is also not able to detect faces when the face is visible, but is upside down. This also happens with faces that are tilted. This is because the Haar cascades are created to identify faces in rectangles, so tilted faces are not detected.\n",
    "5. A good feature is that it is able to detect faces even when they are slightly blurred, but the features are still somewhat visible. This is again because the Haar cascades only look at the sum of pixel intensities in the region and not the texture of the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
